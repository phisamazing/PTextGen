{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"https://github.com/phisamazing/PTextGen/blob/main/Lumimade.ipynb","timestamp":1722542626609},{"file_id":"https://github.com/phisamazing/PTextGen/blob/main/Lunaris.ipynb","timestamp":1722461082143},{"file_id":"https://github.com/oobabooga/text-generation-webui/blob/main/Colab-TextGen-GPU.ipynb","timestamp":1722456000176}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# oobabooga/text-generation-webui\n","\n","After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n","\n","* Project page: https://github.com/oobabooga/text-generation-webui\n","* Gradio server status: https://status.gradio.app/"],"metadata":{"id":"MFQl6-FjSYtY"}},{"cell_type":"code","source":["#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n","\n","#@markdown Press play on the music player that will appear below:\n","%%html\n","<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"],"metadata":{"id":"f7TVVj_z4flw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title 2. Launch the web UI\n","\n","import os\n","from pathlib import Path\n","\n","os.environ.pop('PYTHONPATH', None)\n","\n","if Path.cwd().name != 'text-generation-webui':\n","  print(\"\\033[1;32;1m\\n --> Installing the web UI. This will take a while, but after the initial setup, you can download and test as many models as you like.\\033[0;37;0m\\n\")\n","\n","  !git clone https://github.com/oobabooga/text-generation-webui\n","  %cd text-generation-webui\n","\n","  # Install the project in an isolated environment\n","  !GPU_CHOICE=A \\\n","  USE_CUDA118=FALSE \\\n","  LAUNCH_AFTER_INSTALL=FALSE \\\n","  INSTALL_EXTENSIONS=FALSE \\\n","  ./start_linux.sh\n","\n","# Parameters\n","model_url = \"Lewdiculous/mini-magnum-12b-v1.1-GGUF-IQ-Imatrix\"\n","branch = \"\"\n","command_line_flags = \"--n_ctx 20480\"\n","api = True\n","\n","if api:\n","  for param in ['--api', '--public-api']:\n","    if param not in command_line_flags:\n","      command_line_flags += f\" {param}\"\n","\n","model_url = model_url.strip()\n","if model_url != \"\":\n","    if not model_url.startswith('http'):\n","        model_url = 'https://huggingface.co/' + model_url\n","\n","    # Download the model\n","    url_parts = model_url.strip('/').strip().split('/')\n","    output_folder = f\"{url_parts[-2]}_{url_parts[-1]}\"\n","    branch = branch.strip('\"\\' ')\n","    if branch.strip() not in ['', 'main']:\n","        output_folder += f\"_{branch}\"\n","        !python download-model.py {model_url} --branch {branch}\n","    else:\n","        !python download-model.py {model_url} --specific-file \"mini-magnum-12b-v1.1-Q5_K_M-imat.gguf\"\n","else:\n","    output_folder = \"\"\n","\n","# Start the web UI\n","cmd = f\"./start_linux.sh {command_line_flags} --share\"\n","if output_folder != \"\":\n","    cmd += f\" --model {'mini-magnum-12b-v1.1-Q5_K_M-imat.gguf'}\"\n","\n","!$cmd"],"metadata":{"id":"LGQ8BiMuXMDG"},"execution_count":null,"outputs":[]}]}